# MARVELS â€” Multi-Agent Residual Vision-Enhanced Learning for Symbolic Reasoning

> A novel reinforcement learning algorithm to solve the 3Ã—3 Rubik's Cube **without any search algorithms** (no A*, MCTS, etc.) â€” pure policy learning with multi-agent coordination.

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **Multi-Agent Architecture** | 3 specialized agents (Corner, Edge, Center) each with PPO actor-critic networks |
| **Skill Composer** | Attention-based dynamic policy blending â€” learns *when* to prioritize each agent |
| **Quaternion Encoding** | Geometrically meaningful 270-dim state vector using cubie rotation quaternions |
| **ICM Curiosity** | Intrinsic Curiosity Modules drive exploration in sparse-reward environments |
| **Curriculum Learning** | Starts with 1-move scrambles, increases difficulty as the agent improves |
| **Zero Search** | Pure policy inference at test time â€” no tree search, no backtracking |

## ðŸ— Architecture

```
Cube State (54 stickers)
       â†“
Quaternion Encoder â†’ 270-dim state vector
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Corner   â”‚ â”‚ Edge     â”‚ â”‚ Center   â”‚
â”‚ Agent    â”‚ â”‚ Agent    â”‚ â”‚ Agent    â”‚
â”‚ (PPO+ICM)â”‚ â”‚ (PPO+ICM)â”‚ â”‚ (PPO+ICM)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â†“            â†“            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Skill Composer              â”‚
â”‚  Attention-based policy blending    â”‚
â”‚  w_corner + w_edge + w_center = 1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     Final Policy (18 actions)
```

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ rubiks_env.py          # Complete 3Ã—3 cube simulator (18 moves)
â”œâ”€â”€ quaternion_encoder.py  # Quaternion state encoding (270-dim)
â”œâ”€â”€ agents.py              # 3 Actor-Critic agents + ICM curiosity
â”œâ”€â”€ skill_composer.py      # Attention-based policy blending
â”œâ”€â”€ marvels_trainer.py     # Full PPO training loop with GAE
â”œâ”€â”€ main.py                # Entry point: train + solve demo
â”œâ”€â”€ utils.py               # Logging, saving, visualization
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # This file
```

## ðŸš€ Quick Start

### Prerequisites

- Python 3.10+
- PyTorch 2.0+

### Installation

```bash
pip install -r requirements.txt
```

### Training

```bash
# Full training (default: 200 iterations)
python main.py

# Quick smoke test (5 iterations)
python main.py --episodes 5

# Train on GPU with custom settings
python main.py --mode train --episodes 1000 --device cuda --num-envs 32

# Resume from checkpoint
python main.py --mode train --checkpoint checkpoints/best_model.pt
```

### Solve Demo

```bash
# Solve a scrambled cube (loads best checkpoint)
python main.py --mode solve --scramble 15

# Custom checkpoint
python main.py --mode solve --checkpoint checkpoints/best_model.pt --scramble 10
```

### Evaluation

```bash
# Evaluate solve rate across difficulty levels
python main.py --mode eval --trials 100 --scramble 25
```

## âš™ï¸ Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--mode` | `train` | `train`, `solve`, or `eval` |
| `--episodes` | `200` | Number of training iterations |
| `--device` | `auto` | `auto`, `cpu`, `cuda`, or `mps` |
| `--num-envs` | `16` | Parallel environments for rollouts |
| `--lr` | `3e-4` | Learning rate |
| `--scramble` | `15` | Scramble depth for solve/eval |
| `--seed` | `42` | Random seed |

## ðŸ§  Algorithm Details

### Reward System

```
Total Reward = External + 0.5 Ã— Intrinsic Curiosity

External:
  +100  if cube is solved
  -0.01 per move (efficiency pressure)

Intrinsic (per agent):
  Forward model prediction error (ICM)
```

### Training Pipeline

1. **Collect rollouts** from 16 parallel cube environments
2. **Encode states** via quaternion encoder (288 â†’ 270 dims)
3. **Get agent policies** â€” each agent outputs 18-action probability distribution
4. **Compose policies** â€” attention mechanism blends agent outputs
5. **Compute rewards** â€” external + 0.5 Ã— curiosity
6. **GAE advantages** â€” Î³=0.999, Î»=0.95
7. **PPO update** â€” clip=0.2, entropy=0.01, 4 epochs per batch
8. **Curriculum** â€” increase scramble depth when solve rate > 50%

### 18 Actions

6 faces Ã— 3 rotations = 18 actions:
- **Faces**: U (Up), D (Down), F (Front), B (Back), L (Left), R (Right)
- **Rotations**: CW (90Â°), 180Â°, CCW (270Â°)

## ðŸ“Š Training Output

The training loop prints progress in this format:

```
  Iter    10/200 â”‚ Scramble:  1 â”‚ Solve: 45.0% â”‚ Reward:  +23.45 â”‚ Moves:  12.3 â”‚ ...
  Iter    20/200 â”‚ Scramble:  1 â”‚ Solve: 78.0% â”‚ Reward:  +67.89 â”‚ Moves:   8.1 â”‚ ...
  ðŸ“ˆ Curriculum advanced â†’ scramble depth = 2
  Iter    30/200 â”‚ Scramble:  2 â”‚ Solve: 32.0% â”‚ Reward:  +15.67 â”‚ Moves:  23.4 â”‚ ...
```

## ðŸ“œ License

MIT
